# Maintaining Machine Learning Accuracy with 85% Fewer Features

Analyzed smartphone price data to explore how predictive accuracy can be preserved while drastically reducing model complexity.

## Key Highlights

- Reduced the feature count by 85% (from 20 to just 3) while maintaining 91% classification accuracy  
- Demonstrated how to simplify complex data without sacrificing predictive power  
- Applied data mining techniques (preprocessing, feature selection and extraction, dimensionality reduction) to support model generalization
- Evaluated model accuracy before and after feature simplification
- Focused on optimizing KNN using minimal yet highly informative features (RAM, battery power, screen resolution)  
- Used Python, Pandas, NumPy, Scikit-learn, Matplotlib, and Seaborn  

## Implementation
This notebook is also available and executable on Kaggle:
[View full notebook with data](https://www.kaggle.com/code/shakkutlu/maintaining-ml-accuracy-with-85-fewer-features)
